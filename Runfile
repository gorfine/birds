require 'bundler'
Bundler.require :default
require 'csv'

title   "Bird Scraper"

action :scrape do
  result = {}
  
  (1..26).each do |i|
    doc = html "eng#{i}.html"

    bird_links = doc.css 'h2 a'
    bird_links.each do |link|
      href = link['href']
      next unless href
      resay "#{href}  "
      data = bird_data href
      next unless data
      result[href] = data
    end
  end

  resay "done"

  save_yaml result
  save_csv result
end

action :images do
  data = YAML.load_file 'out.yaml'
  image_keys = %w[image stamp1 stamp2 stamp3 stamp4]
  data.values.each do |bird|
    image_keys.each do |image_key|
      image = bird[image_key]
      file = "images/#{image}"
      next if File.exist? file
      resay "#{file}  "
      system "wget -qO '#{file}' #{base_url}/#{image}"
    end
  end  
end

action :debug do
  data = bird_data "bc1.html"
  File.write "debug.yaml", data.to_yaml
end

class String
  def clean
    gsub(/\s+/, " ").strip
  end
end

def save_yaml(data)
  File.write 'out.yaml', data.to_yaml
end

def save_csv(data)
  header = %w[
    url
    number
    name_hebrew
    name_english
    name_latin
    image
    text_hebrew
    subspecies_distribution
    description
    habitat
    feeding
    breeding
    movements
    status
    israel
    stamp1
    stamp2
    stamp3
    stamp4
  ]

  CSV.open("out.csv", "w") do |csv|
    csv << header
    data.values.each do |bird|
      row = []
      header.each do |col|
        cell = bird[col]
        cell = cell.join "\n" if cell.is_a? Array
        row << cell
      end
      csv << row
    end
  end
end

def bird_data(url)
  page = html url
  return nil if page.to_s.include? "404 Not Found"
  return nil if page.to_s.size < 500

  text = page.css('body').text.clean
  text_data = extract_text text

  text_hebrew = page.css('tr:nth-child(3) > td').text.clean
  matches = text_hebrew.match /(.*)\nsubspecies/im
  text_hebrew = matches.captures.first if matches
  stamps = page.css('h3 img').map { |a| a['src'] }.compact
    .each_with_index.map { |x, i| ["stamp#{i + 1}", x.clean.gsub(' .', '.')] }.to_h

  name_parts = page.css('td')[0].text.clean.split("\n")
  number = name_parts[0].gsub(/[^\d]/, '').to_i
  name_hebrew = name_parts[1]

  base_data = { 
    'url' => url,
    'number' => number,
    'name_hebrew' => name_hebrew,
    'name_english' => page.css('div.fill3').text.clean,
    'name_latin' => page.css('div.fill4').text.clean,
    'image' => page.css('img')[0]['src'].clean,
    'text_hebrew' => text_hebrew,
  }

  base_data.merge(stamps).merge(text_data)
end

def extract_text(text)
  lines = text.gsub("\r", "").split("\n").reject { |a| a.strip.empty? }.map(&:strip)
  section = nil
  result = {}
  
  lines.each do |line|
    case line
    when /^Subspecies and Distribution.?$/i
      section = 'subspecies_distribution'
    when /^descriptive notes.?$/i
      section = 'description'
    when /^habitat.?$/i
      section = 'habitat'
    when /^food and feeding.?$/i
      section = 'feeding'
    when /^breeding.?$/i
      section = 'breeding'
    when /^movements.?$/i
      section = 'movements'
    when /^status and conservation.?$/i
      section = 'status'
    when /^israel.?$/i
      section = 'israel'
    else
      next unless section
      result[section] ||= []
      result[section] << line
    end
  end

  result
end

def base_url
  @base_url ||= "http://israelibirdsstamps.yardbirdsil.info"
end

def html(url)
  Nokogiri::HTML cache.get("#{base_url}/#{url}").content
end

def cache
  @cache ||= WebCache.new life: '7d'
end